# Mise en place Proxmox + Ceph : virtualisation Open Source

![alt text](image.png)

## Qu'est-ce que Proxmox VE ?
Proxmox Virtual Environment (Proxmox VE) est une plateforme open source de virtualisation. Elle permet de g√©rer des machines virtuelles (VM) et des conteneurs, offrant une interface web intuitive pour une administration simplifi√©e. Proxmox VE supporte les hyperviseurs KVM (Kernel-based Virtual Machine) et LXC (Linux Containers), permettant ainsi de d√©ployer facilement des environnements virtualis√©s.

## Qu'est-ce que Ceph ?
Ceph est un syst√®me de stockage distribu√© con√ßu pour offrir une haute performance, une grande √©volutivit√© et une tol√©rance aux pannes. Il permet de stocker des donn√©es de mani√®re redondante et distribu√©e sur plusieurs n≈ìuds, assurant ainsi une haute disponibilit√© et une r√©cup√©ration rapide en cas de panne. Ceph offre plusieurs interfaces de stockage, y compris le stockage d'objets, le stockage de blocs et le syst√®me de fichiers.

## Quelles sont les avantages de l'int√©gration de Proxmox et de Ceph ?

Haute Disponibilit√© : Avec Ceph, les donn√©es sont r√©pliqu√©es sur plusieurs n≈ìuds, ce qui garantit que vos VM et conteneurs restent disponibles m√™me en cas de panne d'un serveur.

Scalabilit√© : Ajouter des ressources de stockage et de calcul est simple et peut √™tre fait sans interruption de service.

Gestion Centralis√©e : L'interface web de Proxmox VE permet de g√©rer facilement l'infrastructure de virtualisation et le stockage Ceph, r√©duisant ainsi la complexit√© administrative.

Performance Optimale : Ceph offre un stockage haute performance, id√©al pour les applications critiques et les charges de travail intensives.

## Mise en place d'un Cluster Proxmox VE avec Ceph

Dans la suite de cet article, nous allons installer et configurer un cluster Proxmox en haute disponibilit√©, int√©grant Ceph pour le stockage distribu√©.

### Doc officielle:

https://pve.proxmox.com/wiki/Deploy_Hyper-Converged_Ceph_Cluster

#### √âtape 1: Installation de Proxmox VE

Installez Proxmox VE sur vos serveurs physiques ou virtuels.

Nous allons installer trois serveurs Proxmox avec des configurations identiques, et avec trois interfaces r√©seau minimum chacun.
![alt text](image-2.png)

Les trois PVE qui vont constituer notre cluster

![alt text](image-3.png)

#### √âtape 2: Configuration du r√©seau

Configurez les adresses IP et les noms d'h√¥tes pour assurer une communication fluide entre les serveurs.

PVE-1 : 192.168.0.111

PVE-2: 192.168.0.112

PVE-3: 192.168.0.113

Sur chaque serveur Proxmox il faudra modifier le fichier /etc/hosts comme suit:

![alt text](image-4.png)

#### Puis nous devrons cr√©er au moins trois r√©seaux bien distincts:

En cas de doute, nous recommandons d'utiliser trois r√©seaux (physiques) distincts pour les configurations √† hautes performances : un r√©seau √† tr√®s haut d√©bit (25 Gbit/s et plus) pour le trafic du cluster Ceph (interne). Un second r√©seau √† haut d√©bit (10 Gbit/s et plus) pour le trafic Ceph (public) entre le serveur Ceph et le trafic de stockage du client Ceph.

## √âtape 3: Initialisation du Cluster

Cr√©ez et joignez les serveurs au cluster Proxmox.

Nous pouvons cr√©er notre cluster en ligne de commande avec:
```bash
pvecm create mon cluster 
```
Ou directement sur le portail web d'un des trois n≈ìuds comme suit:

![alt text](image-5.png)

Le Cluster est cr√©√© sur le PVE-1. Nous devons maintenant joindre le PVE-2 et le PVE-3 au Cluster que nous venons de cr√©er.

Pour ce faire, nous nous connectons sur l‚Äôinterface web de notre PVE-2 qui est sur 192.168.0.112, et nous allons sur ¬´ Datacenter ¬ª > ¬´ Cluster ¬ª > ¬´ Join Cluster ¬ª. En fran√ßais cela donne:

![alt text](image-6.png)

Pour joindre un Cluster Proxmox √† partir du second ou troisi√®me n≈ìud, nous devons au pr√©alable r√©cup√©rer les informations de jonction sur le premier n≈ìud, sur lequel nous avons cr√©√© le Cluster, comme suit : ¬´ Datacenter ¬ª > ¬´ Cluster ¬ª > ¬´ Join Information ¬ª.

![alt text](image-7.png)

On copie le contenu de ¬´ Join information ¬ª pour le coller dans ¬´ Information ¬ª de la fen√™tre de jonction du Cluster sur le PVE-2 et du PVE-3.
![alt text](image-8.png)
![alt text](image-9.png)
![alt text](image-10.png)
Et voila le r√©sultat, notre cluster de trois n≈ìuds tout neuf !
![alt text](image-11.png)

## √âtape 4: Installation de Ceph
Installez Ceph sur chaque serveur du cluster.
![alt text](image-12.png)
![alt text](image-13.png)
![alt text](image-14.png)

## √âtape 5: Configuration de Ceph

A cette √©tape nous allons configurer les Monitors et Managers pour g√©rer l'√©tat et les op√©rations du cluster Ceph.
![alt text](image-16.png)
![alt text](image-17.png)

Etat final recherch√© :)

![alt text](image-18.png)

## √âtape 6: Ajouter des OSDs (Object Storage Daemons)

Les OSDs stockent les donn√©es r√©elles dans le cluster Ceph et participent √† la r√©plication et la r√©cup√©ration des donn√©es.

Sur chaque serveur, ajoutez des OSDs. Vous pouvez utiliser des disques sp√©cifiques ou les partitions existantes.

![alt text](image-19.png)

Etat final recherch√© dans Ceph > OSD:

![alt text](image-20.png)

## √âtape 7: Configuration du pool de stockage Ceph

Cr√©ez un pool de stockage dans Ceph pour organiser et g√©rer les donn√©es, notamment pour le stockage de nos machines virtuelles.

![alt text](image-21.png)
![alt text](image-22.png)

Si on clique sur Avanc√©, nous pouvons voir le calcul du PG:
![alt text](image-23.png)

Le nombre de Placement Groups (PG) dans un cluster Ceph est une variable critique qui affecte la performance et la r√©silience du stockage distribu√©. Le calcul optimal des PG peut aider √† √©quilibrer la charge et √† garantir une gestion efficace des donn√©es. Voici comment calculer le nombre de PG appropri√© pour un pool Ceph.

#### Formule de Calcul des PGs

Avant d'aller plus loin je vous conseille de lire ceci:

https://docs.ceph.com/en/nautilus/rados/operations/placement-groups

La formule recommand√©e pour d√©terminer le nombre de PG est :

Nombre de PG = Nombre d'OSD x 100 / facteur de r√©plication

![alt text](image-24.png)

Cependant, il est important d'ajuster ce nombre pour qu'il soit une puissance de 2 (par exemple, 128, 256, 512, etc.), car Ceph fonctionne plus efficacement avec des valeurs de PG qui sont des puissances de 2.

Imaginons que nous avons 6 OSDs, la formule √† appliquer nous donne:

![alt text](image-25.png)

Pour vous simplifier la tache, il est conseill√© d'activer le PG-Autoscaler.

![alt text](image-26.png)

Notre pool Ceph est automatiquement ajout√© comme stockage dans Proxmox VE. Il sera utilis√© par les VM et conteneurs, offrant ainsi une haute disponibilit√© et une redondance des donn√©es.

![alt text](image-27.png)

### √âtape 8: Configuration de la Haute Disponibilit√© (HA)

Nous allons voir comment configurer la haute disponibilit√© (HA) dans Proxmox. Pour cela, il est n√©cessaire que le stockage des machines virtuelles soit partag√© entre les serveurs du groupe HA.

La haute disponibilit√© dans Proxmox est g√©r√©e via des groupes de haute disponibilit√© auxquels les serveurs appartiennent. Cela permet, par exemple, de regrouper diff√©rents types de serveurs membres d‚Äôun cluster et d'organiser les serveurs de type identique dans un m√™me groupe.

#### Pour cr√©er le groupe HA:
![alt text](image-28.png)

Ensuite nous pouvons int√©grer dans le groupe HA les VM de notre choix:

![alt text](image-29.png)

D'apr√®s le screen de dessus, la VM 100 (srv-web-01) est configur√©e pour √™tre en haute disponibilit√©.

### √âtape 9: Test - Simulation de la panne d'un n≈ìud

![alt text](image-30.png)
VM 101 qui tourne sur le PVE-1

![alt text](image-31.png)
Arr√™t du n≈ìud PVE-1

![alt text](image-32.png)
D√©but du la bascule de la VM 101

![alt text](image-33.png)

Comme vous pouvez le constater, avec la "panne" du n≈ìud PVE-1 notre la machine VM 101 a bascul√©e en quelques secondes sur le n≈ìud PVE-2! üèÜ





Vous voil√† d√©sormais aux commandes d'une infrastructure hyperconverg√©e √† la fois puissante, flexible et ‚Äì cerise sur le g√¢teau ‚Äì quasi gratuite (mis √† part, bien s√ªr, le co√ªt du mat√©riel et le support Pro). ;)

![alt text](image-34.png)

Proxmox coupl√© √† Ceph se d√©marque par son co√ªt quasi-nul et sa flexibilit√© remarquable. Attention cependant, il requiert une certaine expertise technique pour √™tre parfaitement ma√Ætris√©.

De l'autre c√¥t√© de la balance, nous avons des mastodontes comme VMware et Nutanix. Ces solutions sont bard√©es de fonctionnalit√©s avanc√©es et d'un support client √† toute √©preuve, mais √† des co√ªts parfois stratosph√©riques.

En fonction de vos besoins sp√©cifiques; budget, complexit√© de gestion et fonctionnalit√©s requises, vous pouvez choisir la solution HCI qui vous convient le mieux. 

Et laissez-moi vous dire, Proxmox avec Ceph n‚Äôont absolument pas √† rougir devant ces g√©ants du march√© !



Pour plus d'informations sur Proxmox (et pas que) je vous invite √† visiter les blog de Stephane ROBERT üê≥ : https://blog.stephane-robert.info/, un des meilleurs blogs francophone dans le domaine de DevSecOps.